---
phase: 21-chat-interface
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/functions/openrouter-chat/index.ts
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Edge Function returns SSE stream with proper headers (Content-Type: text/event-stream, Cache-Control: no-cache, Connection: keep-alive)"
    - "Edge Function validates JWT token via Supabase auth.getUser()"
    - "Edge Function streams LLM responses in SSE format (data: {\"content\":\"...\"}\\n\\n)"
    - "Edge Function sends [DONE] signal when streaming completes"
    - "Edge Function injects patterns_data context into system prompt"
  artifacts:
    - path: "supabase/functions/openrouter-chat/index.ts"
      provides: "SSE streaming with OpenAI SDK"
      min_lines: 150
    - path: "supabase/functions/_shared/system-prompt.ts"
      provides: "Chat-specific system prompt with climbing expertise"
      min_lines: 20
  key_links:
    - from: "supabase/functions/openrouter-chat/index.ts"
      to: "https://openrouter.ai/api/v1/chat/completions"
      via: "OpenAI SDK with stream: true"
      pattern: "await openai\\.chat\\.completions\\.create.*stream:\\s*true"
    - from: "supabase/functions/openrouter-chat/index.ts"
      to: "supabase.auth.getUser()"
      via: "JWT validation"
      pattern: "supabase\\.auth\\.getUser\\(token\\)"
---

<objective>
Create Edge Function for SSE streaming with JWT auth and climbing-specific context injection.

Purpose: Enable real-time streaming responses from LLM to React client via Server-Sent Events (SSE) with proper authentication and context awareness.
Output: Edge Function (supabase/functions/openrouter-chat/index.ts) that validates JWT, streams OpenAI responses via SSE, and injects pattern analysis context.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@supabase/functions/openrouter-coach/index.ts
@supabase/functions/_shared/cors.ts
@src/hooks/useCoachMessages.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SSE streaming Edge Function</name>
  <files>supabase/functions/openrouter-chat/index.ts</files>
  <action>
    Create supabase/functions/openrouter-chat/index.ts with:

    1. Imports: OpenAI SDK, Supabase client, corsHeaders from _shared/cors.ts
    2. Environment validation: OPENROUTER_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY
    3. Initialize OpenAI client with baseURL: 'https://openrouter.ai/api/v1'
    4. Initialize Supabase client with service role key
    5. Deno.serve handler:
       - Handle OPTIONS preflight with CORS headers
       - Validate Authorization header (Bearer token format)
       - Call supabase.auth.getUser(token) to validate JWT
       - Parse request body: { message: string, patterns_data: PatternAnalysis }
       - Validate message: non-empty string, max 5000 characters
       - Fetch last 20 messages from coach_messages table (for context)
       - Build messages array: system prompt + history + user message
       - Create ReadableStream with TextEncoder
       - Call OpenAI chat.completions.create with stream: true, model: 'google/gemini-2.5-pro'
       - Iterate over response chunks with for await...of
       - Stream each chunk via controller.enqueue(encoder.encode('data: ...\\n\\n'))
       - Send 'data: [DONE]\\n\\n' signal when complete
       - Handle errors: stream error message, finally close controller
       - Return Response with stream and SSE headers (Content-Type: text/event-stream, Cache-Control: no-cache, Connection: keep-alive, corsHeaders)
    6. Message history limit: fetch only last 20 messages (chat_messages order by created_at desc limit 20, then reverse)
    7. System prompt: climbing coach specializing in technique, understands beta/grades/styles, concise and helpful

    Reference pattern from openrouter-coach/index.ts for:
    - JWT validation pattern
    - OpenAI SDK usage
    - Error handling with CORS headers
  </action>
  <verify>
    Verify SSE streaming works: curl -X POST https://xxx.supabase.co/functions/v1/openrouter-chat -H "Authorization: Bearer $JWT" -H "Content-Type: application/json" -d '{"message":"test","patterns_data":{}}' should stream chunks with "data: {"content":"..."}" format and end with "data: [DONE]"
  </verify>
  <done>
    Edge Function streams LLM responses via SSE with proper headers, JWT validation, and message history context (last 20 messages)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create chat system prompt module</name>
  <files>supabase/functions/_shared/system-prompt.ts</files>
  <action>
    Create supabase/functions/_shared/system-prompt.ts with:

    1. Export function getChatSystemPrompt(patterns_data: PatternAnalysis): string
    2. Build system prompt that:
       - Establishes role: climbing coach specializing in technique and climbing movement
       - Explains purpose: helps climbers improve through Q&A about beta, grades, training, styles
       - Injects context: if patterns_data provided, summarize user's weaknesses (e.g., "Struggles with: crimp holds, overhangs")
       - Defines response style: concise, helpful, uses climbing terminology (beta, crimp, sloper, overhang, slab, send, flash, project)
       - Encourages questions: ask clarifying questions if needed
    3. Return string containing the full system prompt

    Example output:
    "You are an expert climbing coach specializing in bouldering and sport climbing. Your expertise covers technique, beta analysis, grade progression, and training methods.

    User's climbing profile (based on pattern analysis):
    - Struggles with: crimp holds (40% fail rate), overhangs (35% fail rate)
    - Climbing frequency: 15 climbs/month

    Provide helpful, concise answers using proper climbing terminology. Ask clarifying questions if needed to understand the user's specific situation."
  </action>
  <verify>
    Verify system-prompt.ts exists and exports getChatSystemPrompt. Check file contains climbing coach role description and context injection logic.
  </verify>
  <done>
    System prompt module provides climbing-specific context-aware prompt for LLM
  </done>
</task>

</tasks>

<verification>
Overall phase checks:
- Edge Function deployed and accessible via /functions/v1/openrouter-chat
- SSE headers verified in response (Content-Type: text/event-stream, Cache-Control: no-cache)
- JWT authentication working (401 on invalid token, 200 on valid token)
- Streaming chunks deliver in real-time (1-2s latency per chunk)
- [DONE] signal sent when streaming completes
- Message history included in LLM context (last 20 messages)
</verification>

<success_criteria>
SSE streaming Edge Function operational with JWT auth, message history context, and climbing-specific system prompt
</success_criteria>

<output>
After completion, create `.planning/phases/21-chat-interface/21-01-SUMMARY.md`
</output>
